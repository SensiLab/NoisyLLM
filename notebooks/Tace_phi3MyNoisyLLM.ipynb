{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "hxpPNm67GVF6",
    "outputId": "dbb852ad-92b3-406a-a05d-88d9817b7fa3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb14c9433c734c6790382a5145b1d671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd173f7f9cb84446a0c44554732ecb76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import accelerate\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from typing import Optional, Union, Dict, Tuple, List, Set\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"The `seen_tokens` attribute is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"`get_max_cache()` is deprecated\")\n",
    "\n",
    "\n",
    "phi3 = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-128k-instruct\",\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"/Users/tmcn0009/Documents/offload_folder\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model = phi3\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")\n",
    "\n",
    "\n",
    "generated_text = pipeline(model=\"microsoft/Phi-3-mini-128k-instruct\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "kqqXVGfia58S",
    "outputId": "34aaca07-5d15-49a6-cd16-747d595e7d72"
   },
   "outputs": [],
   "source": [
    "class NoiseInjector:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.layer_mapping = self._create_layer_mapping()\n",
    "\n",
    "    def _create_layer_mapping(self):\n",
    "        \"\"\"Create a mapping of layer types to their indices\"\"\"\n",
    "        layer_mapping = {\n",
    "            'attention': [],\n",
    "            'ffn': [],\n",
    "            'all': []\n",
    "        }\n",
    "\n",
    "        for name, module in self.model.named_modules():\n",
    "            layer_idx = None\n",
    "            # Extract layer index if present in name\n",
    "            for part in name.split('.'):\n",
    "                if part.isdigit():\n",
    "                    layer_idx = int(part)\n",
    "                    break\n",
    "\n",
    "            if layer_idx is not None:\n",
    "                if 'attention' in name.lower():\n",
    "                    layer_mapping['attention'].append(layer_idx)\n",
    "                elif any(x in name.lower() for x in ['ffn', 'mlp', 'feedforward']):\n",
    "                    layer_mapping['ffn'].append(layer_idx)\n",
    "                layer_mapping['all'].append(layer_idx)\n",
    "\n",
    "        # Remove duplicates and sort\n",
    "        for key in layer_mapping:\n",
    "            layer_mapping[key] = sorted(list(set(layer_mapping[key])))\n",
    "\n",
    "        return layer_mapping\n",
    "\n",
    "    def prepare_inputs(self, prompt: str):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        input_ids = inputs.input_ids.to(self.device)\n",
    "        attention_mask = inputs.attention_mask.to(self.device)\n",
    "        return input_ids, attention_mask\n",
    "    \n",
    "    def validate_config(self, config: dict) -> dict:\n",
    "        \"\"\"Validate configuration parameters\"\"\"\n",
    "        config = config.copy()\n",
    "        \n",
    "        # Validate temperature\n",
    "        if config.get('temperature', 1.0) <= 0:\n",
    "            print(\"Warning: Temperature must be positive. Setting to 1.0\")\n",
    "            config['temperature'] = 1.0\n",
    "            \n",
    "        # Validate top_p\n",
    "        if config.get('top_p', 1.0) <= 0 or config.get('top_p', 1.0) > 1:\n",
    "            print(\"Warning: top_p must be between 0 and 1. Setting to 0.9\")\n",
    "            config['top_p'] = 0.9\n",
    "            \n",
    "        # Validate noise values\n",
    "        noise_keys = ['query_noise', 'key_noise', 'value_noise', 'output_noise']\n",
    "        for key in noise_keys:\n",
    "            if key in config and config[key] < 0:\n",
    "                print(f\"Warning: {key} cannot be negative. Setting to 0\")\n",
    "                config[key] = 0.0\n",
    "                \n",
    "        return config\n",
    "\n",
    "\n",
    "    def get_dropout_layers(self,\n",
    "                         dropout_config: Dict[str, Union[float, List[int], str]]) -> Set[int]:\n",
    "        \"\"\"\n",
    "        Determine which layers to drop based on configuration\n",
    "        \"\"\"\n",
    "        dropout_prob = dropout_config.get('dropout_prob', 0.0)\n",
    "        layer_type = dropout_config.get('layer_type', 'all')\n",
    "        specific_layers = dropout_config.get('specific_layers', None)\n",
    "        mode = dropout_config.get('mode', 'random')\n",
    "        min_layers = dropout_config.get('min_layers', 1)\n",
    "\n",
    "        # Get candidate layers\n",
    "        if specific_layers is not None:\n",
    "            candidate_layers = specific_layers\n",
    "        else:\n",
    "            candidate_layers = self.layer_mapping[layer_type]\n",
    "\n",
    "        total_layers = len(candidate_layers)\n",
    "        max_dropouts = max(0, total_layers - min_layers)\n",
    "\n",
    "        if mode == 'random':\n",
    "            # Randomly select layers to drop based on probability\n",
    "            dropout_layers = set()\n",
    "            for layer in candidate_layers:\n",
    "                if len(dropout_layers) < max_dropouts and random.random() < dropout_prob:\n",
    "                    dropout_layers.add(layer)\n",
    "        else:  # sequential\n",
    "            # Drop layers sequentially from the top\n",
    "            num_dropouts = min(max_dropouts, int(total_layers * dropout_prob))\n",
    "            dropout_layers = set(candidate_layers[-num_dropouts:])\n",
    "\n",
    "        return dropout_layers\n",
    "\n",
    "    def generate_with_noise(self,\n",
    "                          prompt: str,\n",
    "                          noise_config: Dict[str, Union[float, Dict]],\n",
    "                          dropout_config: Dict[str, Union[float, List[int], str]] = None,\n",
    "                          max_length: int = 100,\n",
    "                          **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Generate text with both noise injection and layer dropout\n",
    "        \"\"\"\n",
    "        input_ids, attention_mask = self.prepare_inputs(prompt)\n",
    "        hooks = []\n",
    "\n",
    "        # Get layers to dropout if dropout_config is provided\n",
    "        dropout_layers = set()\n",
    "        if dropout_config is not None:\n",
    "            dropout_layers = self.get_dropout_layers(dropout_config)\n",
    "\n",
    "        def attention_forward_pre_hook(module, inputs):\n",
    "            # Get layer index from module name\n",
    "            layer_idx = None\n",
    "            for part in str(module).split('.'):\n",
    "                if part.isdigit():\n",
    "                    layer_idx = int(part)\n",
    "                    break\n",
    "\n",
    "            # If this layer should be dropped, return zero tensor\n",
    "            if layer_idx in dropout_layers:\n",
    "                zero_tensor = torch.zeros_like(inputs[0])\n",
    "                return (zero_tensor,) + inputs[1:] if len(inputs) > 1 else zero_tensor\n",
    "\n",
    "            # Most transformer implementations pass query, key, value as the first input\n",
    "            if not inputs:\n",
    "                return inputs\n",
    "\n",
    "            hidden_states = inputs[0]\n",
    "\n",
    "            # Get the projection weights\n",
    "            q_proj_weight = getattr(module, 'q_proj', None) or getattr(module, 'query_proj', None) or getattr(module, 'query', None)\n",
    "            k_proj_weight = getattr(module, 'k_proj', None) or getattr(module, 'key_proj', None) or getattr(module, 'key', None)\n",
    "            v_proj_weight = getattr(module, 'v_proj', None) or getattr(module, 'value_proj', None) or getattr(module, 'value', None)\n",
    "\n",
    "            if q_proj_weight is not None and noise_config.get('query_noise', 0) > 0:\n",
    "                query = F.linear(hidden_states, q_proj_weight)\n",
    "                query_noise = torch.randn_like(query) * noise_config['query_noise']\n",
    "                query = query + query_noise\n",
    "                inputs = (query,) + inputs[1:]\n",
    "\n",
    "            if k_proj_weight is not None and noise_config.get('key_noise', 0) > 0:\n",
    "                key = F.linear(hidden_states, k_proj_weight)\n",
    "                key_noise = torch.randn_like(key) * noise_config['key_noise']\n",
    "                key = key + key_noise\n",
    "                if len(inputs) > 1:\n",
    "                    inputs = inputs[:1] + (key,) + inputs[2:]\n",
    "\n",
    "            if v_proj_weight is not None and noise_config.get('value_noise', 0) > 0:\n",
    "                value = F.linear(hidden_states, v_proj_weight)\n",
    "                value_noise = torch.randn_like(value) * noise_config['value_noise']\n",
    "                value = value + value_noise\n",
    "                if len(inputs) > 2:\n",
    "                    inputs = inputs[:2] + (value,) + inputs[3:]\n",
    "\n",
    "            return inputs\n",
    "\n",
    "        def attention_forward_hook(module, inputs, output):\n",
    "            # Get layer index\n",
    "            layer_idx = None\n",
    "            for part in str(module).split('.'):\n",
    "                if part.isdigit():\n",
    "                    layer_idx = int(part)\n",
    "                    break\n",
    "\n",
    "            # If this layer should be dropped, return zeros\n",
    "            if layer_idx in dropout_layers:\n",
    "                if isinstance(output, tuple):\n",
    "                    zero_tensor = torch.zeros_like(output[0])\n",
    "                    return (zero_tensor,) + output[1:]\n",
    "                return torch.zeros_like(output)\n",
    "\n",
    "            # Add noise to the attention output\n",
    "            if isinstance(output, tuple):\n",
    "                attention_output = output[0]\n",
    "            else:\n",
    "                attention_output = output\n",
    "\n",
    "            # Add output noise if specified\n",
    "            if noise_config.get('output_noise', 0) > 0:\n",
    "                noise = torch.randn_like(attention_output) * noise_config['output_noise']\n",
    "                attention_output = attention_output + noise\n",
    "\n",
    "            # Apply temperature scaling\n",
    "            if noise_config.get('temperature', 1.0) != 1.0:\n",
    "                attention_output = attention_output / noise_config['temperature']\n",
    "\n",
    "            if isinstance(output, tuple):\n",
    "                return (attention_output,) + output[1:]\n",
    "            return attention_output\n",
    "\n",
    "        # Register hooks for each attention layer\n",
    "        for name, module in self.model.named_modules():\n",
    "            if \"attention\" in name.lower():\n",
    "                # Add pre-hook for Q/K/V noise\n",
    "                pre_hook = module.register_forward_pre_hook(attention_forward_pre_hook)\n",
    "                hooks.append(pre_hook)\n",
    "\n",
    "                # Add post-hook for output noise\n",
    "                post_hook = module.register_forward_hook(attention_forward_hook)\n",
    "                hooks.append(post_hook)\n",
    "\n",
    "        try:\n",
    "            # Set up generation parameters\n",
    "            generation_config = {\n",
    "                'max_length': max_length,\n",
    "                'do_sample': True,\n",
    "                'pad_token_id': self.tokenizer.pad_token_id,\n",
    "                'eos_token_id': self.tokenizer.eos_token_id,\n",
    "                'top_k': noise_config.get('top_k', 50),\n",
    "                'top_p': noise_config.get('top_p', 1.0),\n",
    "                'temperature': noise_config.get('temperature', 1.0),\n",
    "            }\n",
    "\n",
    "            # Add any additional kwargs\n",
    "            generation_config.update(kwargs)\n",
    "\n",
    "            # Generate with noise injection\n",
    "            outputs = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                **generation_config\n",
    "            )\n",
    "\n",
    "            # Decode the output\n",
    "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        finally:\n",
    "            # Clean up hooks\n",
    "            for hook in hooks:\n",
    "                hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "\n",
    "\n",
    "noise_injector = NoiseInjector(model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sCblylE7kt3v",
    "outputId": "6fd20e95-57b0-4503-a799-7e6ad3b78679"
   },
   "outputs": [],
   "source": [
    "noise_injector = NoiseInjector(model, tokenizer)\n",
    "\n",
    "#temperature must be higher than 0.0\n",
    "\n",
    "noise_config = {\n",
    "    'embedding_noise': 0.0,\n",
    "    'query_noise': 0.0,\n",
    "    'key_noise': 0.0,\n",
    "    'value_noise': 0.0,\n",
    "    'output_noise': 0.0,\n",
    "    #temperature must be higher than 0.0\n",
    "    'temperature': 1.0,\n",
    "    'top_k': 0,  # Disable top-k sampling = 0\n",
    "    'top_p': 1.0 # Disable top-p sampling = 1.0\n",
    "}\n",
    "\n",
    "\n",
    "# Configure dropout parameters\n",
    "dropout_config = {\n",
    "    'dropout_prob': 0.0,        # Probability of dropping a layer\n",
    "    'layer_type': 'all',  # Type of layers to consider ('attention', 'ffn', or 'all')\n",
    "    'mode': 'random',          # Dropout mode ('random' or 'sequential')\n",
    "    'min_layers': 1,           # Minimum number of layers to keep\n",
    "    'specific_layers': [2, 4, 7, 9]    # Optional: To use specific layers for dropout: Set specific_layers=[0, 2, 4] in dropout_config\n",
    "}\n",
    "\n",
    "#To only use noise: Omit the dropout_config parameter\n",
    "# Generate text with both noise and dropout use following\n",
    "generated_text = noise_injector.generate_with_noise(\n",
    "    prompt=\"sad poem\",\n",
    "    noise_config=noise_config,\n",
    "    dropout_config=dropout_config,\n",
    "    max_length=100\n",
    ")\n",
    "\n",
    "generated_text = noise_injector.generate_with_noise(\n",
    "    prompt=\"sad poem\",\n",
    "    noise_config=noise_config,\n",
    "    max_length=80\n",
    "\n",
    ")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "NoiseInjector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
