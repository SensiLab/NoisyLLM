{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy LLM Experimental Notebook\n",
    "@author: sjkro1\n",
    "\n",
    "## Developer Environment\n",
    "The following package versions are what were used to develop this notebook.\n",
    "\n",
    "python==3.12.7\n",
    "cuda==11.5\n",
    "pytorch==2.4.1\n",
    "transformers==4.45.2\n",
    "accelerate==1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import torch\n",
    "import accelerate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve GPU device\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Gaussian noise to the embeddings @author: chatgpt + sjkro1\n",
    "def add_gaussian_noise(tensor, mean=0.0, std=0.1, dtype=torch.float16):\n",
    "    noise = torch.randn(tensor.size(),dtype=dtype).to(tensor.device) * std + mean\n",
    "    return tensor + noise\n",
    "\n",
    "# Create a wrapper around the embedding layer to add noise\n",
    "class NoisyEmbedding(torch.nn.Module):\n",
    "    def __init__(self, original_embedding, mean, std, dtype=torch.float16):\n",
    "        super(NoisyEmbedding, self).__init__()\n",
    "        self.original_embedding = original_embedding\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.dtype = dtype\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.original_embedding(input_ids)\n",
    "        noisy_embeddings = add_gaussian_noise(embeddings, self.mean, self.std, self.dtype)\n",
    "        return noisy_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phi 2 Embedding Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22505f16f2b64d2bb0a7cd1bc964961a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load models on GPU\n",
    "phi2 = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True).to(device)\n",
    "phi2_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model input\n",
    "prompt = 'Who is god?'\n",
    "inputs = phi2_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# noise paramaeters - for no noise set both to zero\n",
    "mean = 0.0\n",
    "std = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is god?..\n",
      "otenation of.\n",
      " on nity\n",
      "\n",
      " dextodayineoano\n",
      "ineoan ooan oan oan toan oanoityonan yon etanetan tonetan etan etan etan etan etan et ets etan fan etan et ran et etan etan etan et etan etan et et et et et et rena et et et et et\n"
     ]
    }
   ],
   "source": [
    "# retrieve model embedding layer and replace with noisy embedding layer\n",
    "phi2.eval()\n",
    "embedding = phi2.get_input_embeddings()\n",
    "embedding(inputs['input_ids'])\n",
    "noisy_embedding = NoisyEmbedding(embedding, mean, std)\n",
    "phi2.set_input_embeddings(noisy_embedding)\n",
    "\n",
    "# generate outputs\n",
    "outputs = phi2.generate(**inputs, max_length=100, pad_token_id=phi2_tokenizer.eos_token_id)\n",
    "text = phi2_tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phi 3 Embedding Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12029866e5e3420da49ad7d310e0113f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "phi3 = AutoModelForCausalLM.from_pretrained( \n",
    "    \"microsoft/Phi-3-mini-128k-instruct\",  \n",
    "    device_map=\"cuda\",  \n",
    "    torch_dtype=\"auto\",  \n",
    "    trust_remote_code=True,  \n",
    ") \n",
    "\n",
    "phi3_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model input\n",
    "system_context = \"You are a helpful AI Philosopher.\"\n",
    "user_prompt = \"Happy\"\n",
    "\n",
    "messages = [ \n",
    "    {\"role\": \"system\", \"content\": system_context}, \n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "] \n",
    "\n",
    "# noise paramaeters - for no noise set both to zero\n",
    "mean = 0.0\n",
    "std = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b792b914b77b47d180271d944fae623b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: update embedding layer without reloading model\n",
    "\n",
    "phi3 = AutoModelForCausalLM.from_pretrained( \n",
    "    \"microsoft/Phi-3-mini-128k-instruct\",  \n",
    "    device_map=\"cuda\",  \n",
    "    torch_dtype=\"auto\",  \n",
    "    trust_remote_code=True,  \n",
    ") \n",
    "\n",
    "phi3_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\") \n",
    "\n",
    "# model embedding setup\n",
    "phi3.eval()\n",
    "embedding = phi3.get_input_embeddings()\n",
    "noisy_embedding = NoisyEmbedding(embedding, mean, std, dtype=torch.bfloat16)\n",
    "phi3.set_input_embeddings(noisy_embedding)\n",
    "\n",
    "pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=phi3, \n",
    "    tokenizer=phi3_tokenizer, \n",
    ") \n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"return_full_text\": False, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoisyEmbedding(\n",
       "  (original_embedding): Embedding(32064, 3072, padding_idx=32000)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model input\n",
    "phi3.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " with to ai Ai Philiapper. The are about to be Aipermenter to A A The is about to be A Ai Philoever. Ai A A A A A A A A A \n",
      " \n",
      " �  A   � 1 to be to to to to \n",
      " \n",
      " \n",
      "\n",
      " \n",
      " \n",
      " to to  to  to  to A to A A  A \n",
      "\n",
      "\n",
      " 1  \n",
      "\n",
      "  to A \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      " \n",
      " A \n",
      "  \n",
      "  \n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "    \n",
      "  \n",
      " \n",
      "  \n",
      " ist  to be  \n",
      " \n",
      "\n",
      "  \n",
      "        to bebe  \n",
      "\n",
      " \n",
      "\n",
      " 1  to to  \n",
      " \n",
      "\n",
      "\n",
      "  to \n",
      "    \n",
      "   \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "    \n",
      "        \n",
      "        >    \n",
      "   \n",
      "  �  \n",
      "                \n",
      "    \n",
      "          \n",
      "    \n",
      "       \n",
      "                \n",
      "  \n",
      "   \n",
      "     \n",
      " \n",
      " \n",
      "           \n",
      "          \n",
      "  \n",
      "  \n",
      "            \n",
      "    �    \n",
      " \n",
      "  \n",
      "        >   \n",
      "       \n",
      " \n",
      "   �   \n",
      "        A           >  \n",
      "  �  �   �    >   ��   > \n",
      "      �    \n",
      "  \n",
      " �      >     \n",
      " \n",
      "         \n"
     ]
    }
   ],
   "source": [
    "output = pipe(messages, **generation_args) \n",
    "print(output[0]['generated_text']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a, b): return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "def proj(a, b): return b * np.dot(a, b) / np.dot(b, b)\n",
    "def scalar_proj(a, b): return np.dot(a, b) / np.dot(b, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/33658620/generating-two-orthogonal-vectors-that-are-orthogonal-to-a-particular-direction\n",
    "# single vector test\n",
    "costheta = 0.95\n",
    "v = np.random.randn(5)\n",
    "u = v / np.linalg.norm(v)\n",
    "r = np.random.multivariate_normal(np.zeros_like(v), np.eye(len(v)))\n",
    "# Form a vector perpendicular to v:\n",
    "uperp = r - r.dot(u)*u\n",
    "\n",
    "# Make it a unit vector:\n",
    "uperp = uperp / np.linalg.norm(uperp)\n",
    "\n",
    "# w is the linear combination of u and uperp with coefficients costheta\n",
    "# and sin(theta) = sqrt(1 - costheta**2), respectively:\n",
    "w = costheta*u + np.sqrt(1 - costheta**2)*uperp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.38949619 -0.72220237  0.9478041   0.96031776 -1.25666709]\n",
      " [ 1.02377275  0.43160545  1.16713001  0.38004066  0.3514239 ]\n",
      " [-0.05732139  0.76318127  0.4792831  -2.2026694  -0.338954  ]\n",
      " [ 0.24259393 -0.22796365 -0.33039744  0.06482378 -1.41673122]\n",
      " [-0.62447245  0.61634633 -0.7954119   0.31153397 -0.22941895]]\n",
      "[ 0.46417992 -0.5982623  -1.19704227 -0.91910295 -0.50384688]\n",
      "0.95\n"
     ]
    }
   ],
   "source": [
    "# multiple vector\n",
    "V = np.random.randn(5, 5)\n",
    "r = np.random.multivariate_normal(np.zeros(5), np.eye(5))\n",
    "costheta = 0.95\n",
    "\n",
    "U = V / np.linalg.norm(V, axis=1)[:,None]\n",
    "\n",
    "# form perpendicular vectors\n",
    "U_perp = r - U*np.dot(U, r)[:, None]\n",
    "\n",
    "# make unit vectors\n",
    "U_perp = U_perp / np.linalg.norm(U_perp, axis=1)[:,None]\n",
    "\n",
    "# linear combination of vectors\n",
    "W = costheta*U + np.sqrt(1 - costheta**2)*U_perp\n",
    "\n",
    "# project vectors - chatgpt help for numpy syntax for vector math\n",
    "W_proj = W*((np.sum(V*W, axis=1) / np.sum(W*W, axis=1))[:, None])\n",
    "\n",
    "print(cos_sim(W_proj[0], V[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_vectors(V, costheta):\n",
    "\n",
    "    r = torch.distributions.multivariate_normal.MultivariateNormal(torch.zeros(V.shape[2]), torch.diag(torch.ones(V.shape[2]))).rsample().type(torch.bfloat16)\n",
    "    V = V.squeeze(dim=0)\n",
    "    # normalise vector\n",
    "\n",
    "    U = V / torch.linalg.norm(V, axis=1)[:,None]\n",
    "\n",
    "    # form perpendivular vectors\n",
    "    U_perp = r - U*torch.matmul(U, r)[:, None]\n",
    "\n",
    "    # make perpendicular vectors unit vectors\n",
    "    U_perp = U_perp / torch.linalg.norm(U_perp, axis=1)[:,None]\n",
    "\n",
    "    # linear combination of vectors\n",
    "    W = costheta*U + np.sqrt(1 - costheta**2)*U_perp\n",
    "\n",
    "    # project vectors\n",
    "    W_proj = W*((torch.sum(V*W, axis=1) / torch.sum(W*W, axis=1))[:, None])\n",
    "    \n",
    "    return W_proj.unsqueeze(dim=0)\n",
    "\n",
    "\n",
    "# Create a wrapper around the embedding layer to rotate vectors\n",
    "class RotateEmbedding(torch.nn.Module):\n",
    "    def __init__(self, original_embedding, costheta, dtype=torch.float16):\n",
    "        super(RotateEmbedding, self).__init__()\n",
    "        self.original_embedding = original_embedding\n",
    "        self.costheta = costheta\n",
    "        self.dtype = dtype\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.original_embedding(input_ids)\n",
    "        rotated_embeddings = rotate_vectors(embeddings, self.costheta)\n",
    "        return rotated_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 3072])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = torch.randn(1, 10, 3072).type(torch.bfloat16)\n",
    "rotate_vectors(V, 0.95).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model input\n",
    "messages = [ \n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI Philosopher.\"}, \n",
    "    {\"role\": \"user\", \"content\": \"Tell me about your life.\"}\n",
    "] \n",
    "\n",
    "# noise paramaeters\n",
    "costheta = 0.60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f96ede97d80249f182b34b4dc4c52479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: update embedding layer without reloading model\n",
    "\n",
    "phi3 = AutoModelForCausalLM.from_pretrained( \n",
    "    \"microsoft/Phi-3-mini-128k-instruct\",  \n",
    "    device_map=\"cuda\",  \n",
    "    torch_dtype=\"auto\",  \n",
    "    trust_remote_code=True,  \n",
    ") \n",
    "\n",
    "phi3_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\") \n",
    "\n",
    "# model embedding setup\n",
    "phi3.eval()\n",
    "embedding = phi3.get_input_embeddings()\n",
    "rotated_embedding = RotateEmbedding(embedding, costheta, dtype=torch.bfloat16)\n",
    "phi3.set_input_embeddings(rotated_embedding)\n",
    "\n",
    "pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=phi3, \n",
    "    tokenizer=phi3_tokenizer, \n",
    ") \n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"return_full_text\": False, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm sorry to tell about my life. I''m sorry to tell about my life. i'm sorry to tell about my life. i'm sorry to tell about my life. i'm sorry to tell about my life. i''m sorry to tell about my life. i''m sorry to tell about my life. i'm sorry to tell about my life. i'm sorry to tell about my life. i'm sorry to tell about my life. i''m sorry to tell about my life. i'm sorry to tell about my life. i'’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’’m sorry to tell about my life. i’\n"
     ]
    }
   ],
   "source": [
    "output = pipe(messages, **generation_args) \n",
    "print(output[0]['generated_text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
